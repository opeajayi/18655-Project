# 18655-Project

### Abstracts
Artificial intelligence research in the area of computer vision teaches machines to
comprehend and interpret visual data. In recent years, there has been significant
development in text-to-image models which combine language and generative
image models, to generate images based on textual prompts. In this project, we
will be using Stable Diffusion (a text-to-image model) for two main tasks. First,
we will generate detailed images conditioned on text descriptions using the model,
and secondly, we will train the model to perform textual inversion using African
contextual images such that when provided with a text prompt, the model can
generate images in the style of the African images used to train the model. Focusing
specifically on the second task, we present an approach to guide the creation of
a text-conditioned image such that it models an African context. Using only 3-5
images representing African concepts, like an object or style, we learn to represent
this concept through new “words” in the embedding space of a frozen text-to-image
model. These “words” can be composed into natural language sentences, guiding
personalized creation in an intuitive way.


### Datasets
